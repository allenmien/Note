	# 机器学习算法--词嵌入

### 直观上认识词嵌入
词嵌入(word embedding)是NLP的流行方法
- 词嵌入类比
1.  v(“国王”)  – v(“王后”) ≈ v(“男”) – v(“女”)
*国王和往后语义的差别主要体现在性别上*
2. v(“英国”) + v(“首都”) ≈ v(“伦敦”)
- 相似词映射到相似方向
1. 基本假设：语义“相似”词的，邻居词，分布类似
2. 倒推：两个词邻居词分布类似 → 两个词语义相
近
3. 举例：
猫 猫 宠物 主人 喂食 蹭 喵
狗 狗 宠物 主人 喂食 咬 汪
==> v(“ 猫”) ≈ v(“ 狗”)
- 词嵌入的优点
词向量
	- 传统one-hot编码
		- 维度高(几千–几万维稀疏向量),数据稀疏
		- 难以计算词之间相似度
		- 难以做模糊匹配
	- 词嵌入
		- 维度低(100 – 500维)
		- 无监督学习,不需去掉停用词(stopwords)
		- 天然有聚类后的效果
		- 连续向量,方便机器学习模型处理
		- 罕见词:“风姿绰约” ≈ “漂亮”
- 最早的词嵌入模型
语义相关会选到比较相似的向量
lda利用词出现在文档（文档噪音多）
词向量是上下文小窗口（两个词，在上下文小窗口经常出现，就会相关）
	- 神经语言模型
	- 输入：一个词的上下文
	- 输出：下个词的概率
	- 目标：最大化预测概率
	- 缺点：
		- 两次矩阵乘,两次非线性 变换
		- 参数多,容易过拟合
		- 优化缓慢,不适合大语料
- Word2vecmo
-
