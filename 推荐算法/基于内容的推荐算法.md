# 基于内容的推荐算法

## 概念

基于内容的推荐算法的主要优势在于无冷启动问题，只要用户产生了初始的历史数据，就可以开始进行推荐的计算。而且随着用户的浏览记录数据的增加，这种推荐一般也会越来越准确。

### 两个关键点

#### 用户喜欢看哪些新闻

用户历史浏览的新闻中”提取”能代表新闻主要内容的关键词，看哪些关键词出现的最多。形成用户的关键词标签。

或者，统计这些新闻所属的领域是哪些。（主题？）

#### 两个新闻内容相似

怎么判断两个新闻内容相似？

能不能提取出两个新闻的关键词，然后对比看它们两的关键词是不是相同的呢？

但是一个新闻可以有好几个关键词，要想全部一样，还是比较困难的。所以我们需要对两个新闻的关键词匹配程度做一个合理的量化。

##### TF-IDF算法

term frequency–inverse document frequency

TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。

- “字词的重要性”：因为查找的是文本的关键词，所以要将文本中最“重要”，或者做最能体现文本内容独特性的那些词语找出来。
- “正比增加”：如果一个词在文本中出现的次数越多，那么我们就越有理由认为该词就属于文本的关键词之一。
- “反比下降”：但是有些词如“中国”、“社会”、“媒体”等词，可能是在各个新闻里都容易出现的高频率词，针对这样的词，我们就需要以一种方式降低它对于单独文档内容的独特性贡献。即若一个词在整个语料库的所有文档里都有出现，那么在计算单个文档的关键词时，我们就会相应地调整该词属于文档关键词的可能性。

TF-IDF算法可以能够返回给我们一组属于某篇文本的”关键词-TFIDF值”的词数对，这些关键词最好地代表了这篇文本的核心内容，而这些关键词的相对于本篇文章的关键程度由它的TFIDF值量化。

对比两篇文本的相似程度：
$$
Similarity(A,B)= \Sigma_{i\in m}TFIDF_A*TFIDF_B
$$
m是两篇文章重合关键词的集合。此即将两篇文本的共同关键词的TFIDF的积全部加在一起，获得最终代表两篇文本的相似度的值。

```
举例：
      刚抓进系统的两个新闻，分别提取出关键词与TFIDF值如下：
      A新闻：“美女模特”：100，“女装”：80，“奔驰”：40
      B新闻：“程序员”：100，“女装”：90，“编程”：30
      两篇文章只有一个共同关键词“女装”，故相似度为：80*90=7200。
```

### 用户关键词喜好

用户看了很多新闻，用户身上产生了很多关键词。

如果拿这么多关键词和新闻的关键词一个一个的去对比，会非常的慢。

#### 喜好关键词表

我们为每个用户在数据库里维持一个map，这个map里放的都是“用户喜好的关键词-喜好程度”这样的Key-Value对。

跟踪某用户的浏览行为，每当该用户新浏览了一条新闻，我们就把该新闻的“关键词-TFIDF值”“插入”到该用户的喜好关键词表中。

当然这个“插入”要考虑关键词表里已经预先有了某预插入的关键词的情况，那么在这个基础上，我们可以将预插入的关键词的TFIDF值直接和词表里的值加起来。

### 兴趣迁移——衰减机制

**问题：**

比如这段时间苹果出了一款新产品，我关注一下，但一个月后，我可能就完全不在意这件事了，但是可能苹果相关的关键词还一直在我的关键词表里，那会不会导致我依然收到相似的我已经不关心的新闻的推荐呢？也就是如何处理这种兴趣迁移问题呢？

**衰减机制**

即让用户的关键词表中的每个关键词喜好程度都按一定周期保持衰减。考虑到不同词的TFIDF值可能差异已经在不同的数量级，我们考虑用指数衰减的形式来相对进行公平的衰减。即引入一个λ系数，1>λ>0，我们每隔一段时间，对所有用户的所有关键词喜好程度进行*λ的衰减，那么就完成了模拟用户兴趣迁移的过程。

